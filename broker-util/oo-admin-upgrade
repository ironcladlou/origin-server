#!/usr/bin/env oo-ruby
require 'rubygems'
require 'pp'
require 'thread'
require 'getoptlong'
require 'stringio'
require 'set'
require 'json'
require 'thor'

WORK_DIR = '/tmp/oo-upgrade'
STDOUT.sync, STDERR.sync = true

class Upgrader
  ##
  # The main upgrade controller.
  #
  # 1. Build the node itinerary
  # 2. Create workers to process each node via +upgrade_node+
  # 3. Collect and report upon the results
  #
  # Expects an argument hash:
  #
  #  :version - the upgrade version
  #
  #  :mode    - one of the following:
  #
  #             :normal   - All existing upgrade data is cleared, and a full upgrade
  #                         is initiated.
  #
  #             :continue - All existing upgrade data is preserved and the upgrade
  #                         proceeds using the existing itinerary.
  #
  #             :rerun    - Any existing itinerary is replaced with a new one containing
  #                         only the errors from the previous run; the error results from
  #                         the previous are archived, and will contain new errors resulting
  #                         from the rerun.
  #
  #  :ignore_cartridge_version - ??
  #  :target_server_itentity - only process the specified node regardless of the itinerary
  #  :upgrade_position: ??
  #  :num_upgraders: ??
  def upgrade(args={})
    defaults = {
      version: nil,
      ignore_cartridge_version: false,
      target_server_identity: nil,
      upgrade_position: 1,
      num_upgraders: 1,
      max_threads: 12
    }
    opts = defaults.merge(args) {|k, default, arg| arg.nil? ? default : arg}

    puts "Performing upgrade with options: #{opts.inspect}"

    version = opts[:version]
    ignore_cartridge_version = opts[:ignore_cartridge_version]
    target_server_identity = opts[:target_server_identity]
    upgrade_position = opts[:upgrade_position]
    num_upgraders = opts[:num_upgraders]
    max_threads = opts[:max_threads]

    FileUtils.mkdir_p WORK_DIR if not Dir.exists?(WORK_DIR)

    upgrade_result = {
      times: {},
      gear_count: 0,
      gear_counts: []
    }

    start_time = (Time.now.to_f * 1000).to_i

    # build the itinerary, only writing them to disk if this is a normal
    # mode run (for continue and reruns, we'll be working from existing
    # itineraries at the gear level)
    if not File.exists?(cluster_itinerary_path)
      puts "Building cluster itinerary and metadata"
      create_cluster_itinerary(target_server_identity: target_server_identity,
                               upgrade_position: upgrade_position,
                               num_upgraders: num_upgraders,
                               version: version,
                               ignore_cartridge_version: ignore_cartridge_version)
    else
      puts "Reusing existing cluster itinerary and metadata"
    end

    raise "Couldn't find cluster itinerary at #{cluster_itinerary_path}" unless File.exists?(cluster_itinerary_path)
    raise "Couldn't find cluster metadata at #{cluster_metadata_path}" unless File.exists?(cluster_metadata_path)
    
    puts "Loading cluster metadata from #{cluster_metadata_path}"
    metadata = JSON.parse(IO.read(cluster_metadata_path))

    puts "Loading cluster itinerary from #{cluster_itinerary_path}"
    cluster_itinerary = JSON.parse(IO.read(cluster_itinerary_path))
    cluster_itinerary_queue = JSON.parse(IO.read(cluster_itinerary_path))

    upgrade_result[:cluster_itinerary] = cluster_itinerary
    upgrade_result[:incomplete_cluster_itinerary] = []
    upgrade_result[:metadata] = metadata

    upgrader_position_nodes = metadata['upgrader_position_nodes']

    puts "#####################################################"
    if !upgrader_position_nodes.empty?
      puts 'Nodes this upgrader is handling:'
      puts upgrader_position_nodes.pretty_inspect
    end
    puts "#####################################################"

    node_threads = []
    mutex = Mutex.new
    starting_nodes = cluster_itinerary_queue.shift(max_threads)
    starting_nodes.each_with_index do |node, index|
      server_identity = node['server_identity']
      upgrade_result[:gear_counts][index] = node['gears_length']
      upgrade_result[:gear_count] += node['gears_length']
      node_threads << Thread.new do
        # perform the node upgrade
        upgrade_node(server_identity)
        upgrade_result[:incomplete_cluster_itinerary] << node if node_has_remaining_work?(server_identity)

        # get the next available node to process
        while !cluster_itinerary_queue.empty? do
          server_identity = nil
          mutex.synchronize do
            unless cluster_itinerary_queue.empty?
              node = cluster_itinerary_queue.delete_at(0)
              server_identity = node['server_identity']
              upgrade_result[:gear_counts][index] += node['gears_length']
              upgrade_result[:gear_count] += node['gears_length']
              puts "#####################################################"
              puts "Remaining node queue:"
              puts cluster_itinerary_queue.pretty_inspect
              puts "#####################################################"
            end
          end
          # perform the node upgrade
          upgrade_node(server_identity) if server_identity
          upgrade_result[:incomplete_cluster_itinerary] << node if node_has_remaining_work?(server_identity)
        end
      end
    end

    # wait for all the threads to finish
    node_threads.each do |t|
      t.join
    end

    # rewrite the cluster itinerary if there are any remaining incomplete nodes
    FileUtils.rm_f cluster_itinerary_path if File.exists?(cluster_itinerary_path)
    puts "Writing updated cluster itinerary to #{cluster_itinerary_path}"
    incomplete_itinerary = upgrade_result[:incomplete_cluster_itinerary] + cluster_itinerary_queue
    append_to_file(cluster_itinerary_path, JSON.dump(incomplete_itinerary))

    upgrade_result[:times]["total"] = (Time.now.to_f * 1000).to_i - start_time

    puts build_text_summary(upgrade_result)
  end

  def node_has_remaining_work?(server_identity)
    return File.exists?(node_itinerary_path(server_identity)) || File.exists?(rerun_itinerary_path(server_identity))
  end


  ##
  # Finds all the nodes containing gears to be upgraded.
  #
  # During the course of the gear detection, writes the gears to
  # be upgraded to a file (see +write_node_itinerary+ for details).
  #
  # If the +write_node_itineraries+ argument is +false+, the +write_node_itinerary+ call
  # is skipped.
  #
  # Returns an itinerary hash containing node-level metadata for the upgrade (not including
  # the actual gear details). The nodes are sorted by the number of active gears they contain.
  def create_cluster_itinerary(opts)
    raise "Cluster itinerary file already exists at #{cluster_itinerary_path}" if File.exists?(cluster_itinerary_path)

    target_server_identity = opts[:target_server_identity]
    upgrade_position = opts[:upgrade_position]
    num_upgraders = opts[:num_upgraders]
    version = opts[:version]
    ignore_cartridge_version = opts[:ignore_cartridge_version]

    metadata = {
      logins_count: 0,
      upgrader_position_nodes: [],
      times: {}
    }

    puts "Getting all active gears..."
    gather_active_gears_start_time = (Time.now.to_f * 1000).to_i
    active_gears_map = OpenShift::ApplicationContainerProxy.get_all_active_gears
    metadata[:times]["gather_active_gears_total_time"] = (Time.now.to_f * 1000).to_i - gather_active_gears_start_time

    puts "Getting all logins..."
    gather_users_start_time = (Time.now.to_f * 1000).to_i
    query = {"group_instances.gears.0" => {"$exists" => true}}
    options = {:fields => [ "uuid",
                "domain_id",
                "name",
                "created_at",
                "component_instances.cartridge_name",
                "component_instances.group_instance_id",
                "group_instances._id",
                "group_instances.gears.uuid",
                "group_instances.gears.server_identity",
                "group_instances.gears.name"], 
               :timeout => false}

    ret = []
    user_map = {}
    OpenShift::DataStore.find(:cloud_users, {}, {:fields => ["_id", "uuid", "login"], :timeout => false}) do |hash|
      metadata[:logins_count] += 1
      user_uuid = hash['uuid']
      user_login = hash['login']
      user_map[hash['_id'].to_s] = [user_uuid, user_login]
    end

    domain_map = {}
    OpenShift::DataStore.find(:domains, {}, {:fields => ["_id" , "owner_id"], :timeout => false}) do |hash|
      domain_map[hash['_id'].to_s] = hash['owner_id'].to_s
    end

    node_to_gears = {}
    OpenShift::DataStore.find(:applications, query, options) do |app|
      print '.'
      user_id = domain_map[app['domain_id'].to_s]
      if user_id.nil?
        relocated_domain = Domain.where(_id: Moped::BSON::ObjectId(app['domain_id'])).first
        next if relocated_domain.nil?
        user_id = relocated_domain.owner._id.to_s
        user_uuid = user_id
        user_login = relocated_domain.owner.login
      else
        if user_map.has_key? user_id
          user_uuid,user_login = user_map[user_id]
        else
          relocated_user = CloudUser.where(_id: Moped::BSON::ObjectId(user_id)).first
          next if relocated_user.nil?
          user_uuid = relocated_user._id.to_s
          user_login = relocated_user.login
        end
      end

      app['group_instances'].each do |gi|
        gi['gears'].each do |gear|
          server_identity = gear['server_identity']
          if server_identity && (!target_server_identity || (server_identity == target_server_identity))
            node_to_gears[server_identity] = [] unless node_to_gears[server_identity] 
            node_to_gears[server_identity] << {:server_identity => server_identity, :uuid => gear['uuid'], :name => gear['name'], :app_name => app['name'], :login => user_login}
          end
        end
      end
    end

    metadata[:times]["gather_users_total_time"] = (Time.now.to_f * 1000).to_i - gather_users_start_time

    position = upgrade_position - 1
    if num_upgraders > 1
      server_identities = node_to_gears.keys.sort
      server_identities.each_with_index do |server_identity, index|
        if index == position
          metadata[:upgrader_position_nodes] << server_identity
          position += num_upgraders
        else
          node_to_gears.delete(server_identity)
        end
      end
    end

    # populate the node queue in the itinerary, and write the itinerary to disk
    # if necessary, taking care to write the most active gears first
    node_queue = []
    node_to_gears.each do |server_identity, gears|
      node_to_gears[server_identity] = nil
      break if gears.empty?

      # build the node for the queue with just metadata about the
      # node and gears
      node = {
        server_identity: server_identity,
        version: version,
        ignore_cartridge_version: ignore_cartridge_version,
        active_gear_length: 0,
        inactive_gear_length: 0
      }

      # sort gears by active/inactive
      active_gears = []
      inactive_gears = []

      gears.each do |gear|
        if active_gears_map.include?(server_identity) && active_gears_map[server_identity].include?(gear[:uuid])
          gear[:active] = true
          active_gears << gear
        else
          gear[:active] = false
          inactive_gears << gear
        end
      end

      node[:active_gear_length] = active_gears.length
      node[:inactive_gear_length] = inactive_gears.length
      node[:gears_length] = gears.length

      node_queue << node

      # ensure we can process active gears first
      write_node_itinerary(node, active_gears) unless active_gears.empty?
      write_node_itinerary(node, inactive_gears) unless inactive_gears.empty?
    end
    node_to_gears.clear

    # process the largest nodes first
    node_queue = node_queue.sort_by { |node| node[:active_gear_length] }

    puts "Writing cluster itinerary to #{cluster_itinerary_path}"
    append_to_file(cluster_itinerary_path, JSON.dump(node_queue))

    puts "Writing cluster metadata to #{cluster_metadata_path}"
    append_to_file(cluster_metadata_path, JSON.dump(metadata))
  end


  ##
  # Writes a node itinerary to a file in a consolidated format containing
  # one line per gear, where each line holds the information for both
  # the node and the gear so that future processing can be done on a
  # line basis and have access to all necessary context. The hash is
  # JSON representation of the following hash:
  #
  #   { 
  #     "server_identity": <string>,
  #     "version": <string>,
  #     "ignore_cartridge_version": <string>,
  #     "gear_uuid": <string>,
  #     "gear_name": <string>,
  #     "app_name": <string>,
  #     "login": <string>,
  #     "active": <bool>
  #   }
  def write_node_itinerary(node, gears)
    itinerary_file = node_itinerary_path(node[:server_identity])
    puts "Writing #{gears.length} gears for node #{node[:server_identity]} to file #{itinerary_file}"
    
    gears.each do |gear|
      data = {
        server_identity: node[:server_identity],
        version: node[:version],
        ignore_cartridge_version: node[:ignore_cartridge_version],
        gear_uuid: gear[:uuid],
        gear_name: gear[:name],
        app_name: gear[:app_name],
        login: gear[:login],
        active: gear[:active]
      }
      append_to_file(itinerary_file, JSON.dump(data))
    end
  end

  ##
  # Performs the a node upgrade of the specified +server_identity+ by setting up the output
  # files and forking a call back to +upgrade_node_from_itinerary+.
  #
  # The upgrade logic will continue or retry depending on the presence of the appropriate
  # itinerary files, and will do nothing if there are no itineraries (or if both itinerary
  # types are detected, which is an illegal state).
  def upgrade_node(server_identity)
    raise "No server identity specified" unless server_identity

    timestamp = Time.now.strftime("%Y-%m-%d-%H%M%S")
    
    itinerary_file = node_itinerary_path(server_identity)
    rerun_file = rerun_itinerary_path(server_identity)
    error_file = error_file_path(server_identity)

    # if there's no itinerary of any sort, there's nothing to do on this node
    unless File.exists?(itinerary_file) || File.exists(rerun_file)
      puts "Nothing to migrate for node #{server_identity}; no itinerary files exist"
      return
    end

    if File.exists?(itinerary_file) && File.exists?(rerun_file)
      puts "Warning: node #{server_identity} has both an itinerary and a rerun itinerary. This is an unexpected state and the node will be skipped."
      return
    end

    if File.exists?(itinerary_file)
      puts "Continuing upgrade of node #{server_identity} from existing itinerary file at #{itinerary_file}"
    elsif File.exists?(rerun_file)
      puts "Re-running failures from previous upgrade of node #{server_identity} from rerun itinerary at #{rerun_file}"
      
      archive_error_file = "#{timestamp}_#{error_file}"
      puts "Archiving previous error file from #{error_file} to #{archive_error_file}"
      FileUtils.mv error_file, archive_error_file

      # convert the rerun itinerary into the new normal itinerary
      FileUtils.mv rerun_file, itinerary_file
    end

    # fork the call to +upgrade_node_from_itinerary+ (TODO: revisit this later; the fork is apparently used
    # to work around long-forgotten MCollective threading issues)
    upgrade_node_cmd = "#{__FILE__} upgrade-from-file --upgrade-file '#{itinerary_file}'"
    execute_script(upgrade_node_cmd)
  end

  ##
  # Process JSON entries written by +write_node_itinerary+ and upgrades each gear
  # from the file using +upgrade_gear+.
  #
  # Upgrade results are written as JSON to +results_file+. Results
  # containing errors are written as JSON to +error_file+.
  #
  # When a result contains errors, the source itinerary entry is re-written to
  # +rerun_itinerary_file+ to facilitate reruns of past errors.
  #
  # Each time an result is written to disk, the source line from +file+ is deleted. When
  # all lines are processed, the input file is deleted.
  #
  # This method returns nothing; callers must inspect the result file contents for
  # upgrade details.
  def upgrade_node_from_itinerary(file)
    while true
      itinerary_str = File.open(file, &:gets)
      break if itinerary_str.nil? || itinerary_str.empty?

      data = JSON.load(itinerary_str)
      server_identity = data["server_identity"]
      gear_uuid = data["gear_uuid"]
      gear_name = data["gear_name"]
      app_name = data["app_name"]
      login = data["login"]
      version = data["version"]
      ignore_cartridge_version = data["ignore_cartridge_version"]

      log_file = log_file_path(server_identity)
      results_file = results_file_path(server_identity)
      error_file = error_file_path(server_identity)

      append_to_file(log_file,  "Migrating app '#{app_name}' gear '#{gear_name}' with uuid '#{gear_uuid}' on node '#{server_identity}' for user: #{login}")

      # start the upgrades in a retry loop
      num_tries = 2
      (1..num_tries).each do |i|
        # perform the upgrade
        gear_result = upgrade_gear(login, app_name, gear_uuid, version, ignore_cartridge_version)

        # write the results if all is well
        if gear_result[:errors].empty?
          append_to_file(results_file, JSON.dump(gear_result))
          break
        end

        # dump the error to disk if the retry limit is hit and we're still failing
        if i == num_tries
          gear_result[:errors] << "Failed upgrade after #{num_tries} tries"
          append_to_file(error_file, JSON.dump(gear_result))
          append_to_file(rerun_itinerary_file, itinerary_str)
          break
        end

        # verify the user still exists
        user = nil
        begin
          user = CloudUser.with(consistency: :eventual).find_by(login: login)
        rescue Mongoid::Errors::DocumentNotFound
        end

        # if not, throw a warning and move on
        unless user && Application.find_by_user(user, app_name)
          gear_result[:warnings] << "App '#{app_name}' no longer found in datastore with uuid '#{gear_uuid}'.  Ignoring..." 
          append_to_file(results_file, JSON.dump(gear_result))
          break
        end

        # if so, we're ready for a retry
        sleep 4
      end

      # the gear has been processed; delete the entry from the file
      `sed -i '1,1d' #{file}`
    end

    # double-check to ensure all lines were processed, and remove
    # the input file if we're all done.
    FileUtils.rm_f file if `wc -l #{file}`.to_i == 0
  end


  ##
  # Performs a gear upgrade via an RPC call to MCollective on a remote node.
  #
  # Returns a hash of result data, including the remote upgrade JSON containing
  # the gear level upgrade details.
  #
  # NOTE: all exceptions are trapped and added to the +errors+ array within the
  # results hash. This method should always return the hash.
  def upgrade_gear(login, app_name, gear_uuid, version, ignore_cartridge_version=false)
    total_upgrade_gear_start_time = (Time.now.to_f * 1000).to_i

    # TODO: time for a class?
    gear_result = {
      login: login,
      app_name: app_name,
      gear_uuid: gear_uuid,
      version: version,
      errors: [],
      warnings: [],
      times: {},
      upgrade_result: nil,
      exit_code: nil
    }

    begin
      user = nil
      begin
        user = CloudUser.with(consistency: :eventual).find_by(login: login)
      rescue Mongoid::Errors::DocumentNotFound
      end

      raise "User not found: #{login}" unless user

      app, gear = Application.find_by_gear_uuid(gear_uuid)

      gear_result[:warnings] << "App '#{app_name}' not found" unless app
      gear_result[:warnings] << "Gear not found with uuid #{gear_uuid} for app '#{app_name}' and user '#{login}'" unless gear

      if app && gear
        server_identity = gear.server_identity

        Timeout::timeout(420) do
          output = ''
          exit_code = 1
          upgrade_result_json = nil

          upgrade_on_node_start_time = (Time.now.to_f * 1000).to_i

          OpenShift::MCollectiveApplicationContainerProxy.rpc_exec('openshift', server_identity) do |client|
            client.upgrade(:uuid => gear_uuid,
                           :namespace => app.domain.namespace,
                           :version => version,
                           :ignore_cartridge_version => ignore_cartridge_version.to_s) do |response|
              exit_code = response[:body][:data][:exitcode]
              upgrade_result_json = response[:body][:data][:upgrade_result_json]
            end
          end

          gear_result[:upgrade_result] = JSON.load(upgrade_result_json)
          gear_result[:exit_code] = exit_code

          unless gear_result[:upgrade_result]['upgrade_complete']
            gear_result[:errors] << "Gear upgrade result is marked incomplete"
          end

          upgrade_on_node_time = (Time.now.to_f * 1000).to_i - upgrade_on_node_start_time

          gear_result[:times]["time_upgrade_on_node_measured_from_broker"] = upgrade_on_node_time

          if exit_code != 0
            raise "Remote upgrade returned nonzero exit code: #{exit_code}"
          end
        end
      end
    rescue Timeout::Error => e
      gear_result[:errors] << "Remote upgrade timed out"
    rescue => e
      gear_result[:errors] << "Upgrade failed: #{e.message}"
    end

    total_upgrade_gear_time = (Time.now.to_f * 1000).to_i - total_upgrade_gear_start_time
    gear_result[:times]["time_total_upgrade_gear_measured_from_broker"] = total_upgrade_gear_start_time

    gear_result
  end

  def build_text_summary(upgrade_result)
    buf = []

    leftover_gears_per_node = {}
    failed_gears_per_node = {}

    upgrade_result[:cluster_itinerary].each do |node|
      server_identity = node['server_identity']
      node_itinerary = node_itinerary_path(server_identity)
      rerun_itinerary = rerun_itinerary_path(server_identity)

      if File.exists?(node_itinerary)
        leftover_gears_per_node[server_identity] = `wc -l #{node_itinerary}`.to_i
      end

      if File.exists?(rerun_itinerary)
        failed_gears_per_node[server_identity] = `wc -l #{rerun_itinerary}`.to_i
      end
    end

    unless failed_gears_per_node.empty?
      buf << "!!!!!!!!!!WARNING!!!!!!!!!!!!!WARNING!!!!!!!!!!!!WARNING!!!!!!!!!!"
      buf << "Leftover gears found in node upgrade itineraries:"
      failed_gears_per_node.each do |server_identity, count|
        buf << "  #{server_identity}: #{count}"
      end
      buf << "You can run the upgrade again to continue"
      buf << "!!!!!!!!!!WARNING!!!!!!!!!!!!!WARNING!!!!!!!!!!!!WARNING!!!!!!!!!!"
    end

    unless failed_gears_per_node.empty?
      buf << "!!!!!!!!!!WARNING!!!!!!!!!!!!!WARNING!!!!!!!!!!!!WARNING!!!!!!!!!!"
      buf << "Failed gears found in node rerun itineraries:"
      failed_gears_per_node.each do |server_identity, count|
        buf << "  #{server_identity}: #{count}"
      end
      buf << "You can run the upgrade again to rerun the failed gears"
      buf << "!!!!!!!!!!WARNING!!!!!!!!!!!!!WARNING!!!!!!!!!!!!WARNING!!!!!!!!!!"
    end 

    failed_gears_total = failed_gears_per_node.values.inject{|sum,x| sum + x} || 0
    upgrader_position_nodes = upgrade_result[:metadata]['upgrader_position_nodes']

    buf << "#####################################################"
    buf << "Summary:"
    buf << "# of users: #{upgrade_result[:metadata]['logins_count']}"
    buf << "# of gears: #{upgrade_result[:gear_count]}"
    buf << "# of failures: #{failed_gears_total}"
    buf << "Gear counts per thread: #{upgrade_result[:gear_counts].pretty_inspect}"
    buf << "Nodes upgraded: #{upgrader_position_nodes.pretty_inspect}" if !upgrader_position_nodes.empty?
    buf << "Timings:"
    upgrade_result[:times].each do |topic, time_in_millis|
      buf << "    #{topic}=#{time_in_millis.to_f/1000}s"
    end
    buf << "Additional timings:"
    upgrade_result[:metadata]['times'].each do |topic, time_in_millis|
      buf << "    #{topic}=#{time_in_millis.to_f/1000}s"
    end
    buf << "#####################################################"

    buf.join("\n")
  end

  def log_file_path(server_identity)
    "#{WORK_DIR}/upgrade_log_#{server_identity}"
  end

  def node_itinerary_path(server_identity)
    "#{WORK_DIR}/upgrade_node_itinerary_#{server_identity}"
  end

  def rerun_itinerary_path(server_identity)
    "#{WORK_DIR}/upgrade_node_rerun_itinerary_#{server_identity}"
  end

  def results_file_path(server_identity)
    "#{WORK_DIR}/upgrade_results_#{server_identity}"
  end

  def error_file_path(server_identity)
    "#{WORK_DIR}/upgrade_errors_#{server_identity}"
  end

  def cluster_itinerary_path
    "#{WORK_DIR}/upgrade_cluster_itinerary"
  end

  def cluster_metadata_path
    "#{WORK_DIR}/upgrade_cluster_metadata"
  end

  ##
  # Appends +value+ to +filename+.
  def append_to_file(filename, value)
    FileUtils.touch filename unless File.exists?(filename)

    file = File.open(filename, 'a')
    begin
      file.puts value
    ensure
      file.close
    end
  end

  ##
  # Executes +cmd+ up to +num_tries+ times waiting for a zero exitcode up with
  # a timeout of +timeout+.
  #
  # Returns [+output+, +exitcode+] of the process.
  def execute_script(cmd, num_tries=1, timeout=28800)
    exitcode = nil
    output = ''
    (1..num_tries).each do |i|
      pid = nil
      begin
        Timeout::timeout(timeout) do
          read, write = IO.pipe
          pid = fork {
            # child
            $stdout.reopen write
            read.close
            exec(cmd)
          }
          # parent
          write.close
          read.each do |line|
            output << line
          end
          Process.waitpid(pid)
          exitcode = $?.exitstatus
        end
        break
      rescue Timeout::Error
        begin
          Process.kill("TERM", pid) if pid
        rescue Exception => e
          puts "execute_script: WARNING - Failed to kill cmd: '#{cmd}' with message: #{e.message}"
        end
        puts "Command '#{cmd}' timed out"
        raise if i == num_tries
      end
    end
    return output, exitcode
  end
end

class UpgraderCli < Thor
  no_tasks do
    def with_upgrader
      # Disable analytics for admin scripts
      require '/var/www/openshift/broker/config/environment'
      Rails.configuration.analytics[:enabled] = false
      Rails.configuration.msg_broker[:rpc_options][:disctimeout] = 20

      begin
        upgrader = Upgrader.new
        yield upgrader
      rescue => e
        puts e.message
        puts e.backtrace.join("\n")
        raise
      end
    end
  end

  desc "upgrade-gear", "Upgrades only the specified gear"
  method_option :login, :type => :string, :required => true, :desc => "User login"
  method_option :app_name, :type => :string, :required => true, :desc => "App name of the gear to upgrade"
  method_option :upgrade_gear, :type => :string, :required => true, :desc => "Gear uuid of the single gear to upgrade"
  method_option :version, :type => :string, :required => true, :desc => "Target version number"
  method_option :ignore_cartridge_version, :type => :boolean, :default => false, :desc => "Force cartridge upgrade even if cartridge versions match"
  def upgrade_gear
    with_upgrader do |upgrader|
      gear_result = upgrader.upgrade_gear(options.login, options.app_name, options.upgrade_gear, options.version, options.ignore_cartridge_version?)
      puts JSON.dump(gear_result)
    end
  end

  desc "upgrade-from-file", "Upgrades gears from a node itinerary file"
  method_option :upgrade_file, :type => :string, :required => true, :desc => "The node itinerary file to upgrade from"
  def upgrade_from_file
    with_upgrader do |upgrader|
      upgrader.upgrade_node_from_itinerary(options.upgrade_file)
    end
  end

  desc "upgrade-node", "Upgrades one or all nodes to the specified version"
  method_option :version, :type => :string, :required => true, :desc => "Target version number"
  method_option :ignore_cartridge_version, :type => :boolean, :default => false, :desc => "Force cartridge upgrade even if cartridge versions match"
  method_option :upgrade_node, :type => :string, :required => false, :desc => "Server identity of the node to upgrade"
  method_option :upgrade_position, :type => :numeric, :required => false, :desc => "Postion of this upgrader (1 based) amongst the num of upgraders"
  method_option :num_upgraders, :type => :numeric, :required => false, :desc => "The total number of upgraders to be run.  Each upgrade-position will be a "\
                                                                                "upgrade-position of num-upgraders.  All positions must to taken to upgrade "\
                                                                                "all gears."
  method_option :max_threads, :type => :numeric, :required => false, :desc =>  "Indicates the number of processing queues"
  def upgrade_node
    with_upgrader do |upgrader|
      args = {
        version: options.version,
        ignore_cartridge_version: options.ignore_cartridge_version?,
        target_server_identity: options.target_server_identity,
        upgrade_position: options.upgrade_position,
        num_upgraders: options.num_upgraders,
        max_threads: options.max_threads
      }

      upgrader.upgrade(args)
    end
  end
end

UpgraderCli.start
